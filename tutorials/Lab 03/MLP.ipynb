{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mipSoOVlavkb"
   },
   "source": [
    "# Categorical data with multilayer perceptron (MLP)\n",
    "\n",
    "## Authors: \n",
    "\n",
    "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
    "\n",
    "Gaétan Marceau Caron <gaetan.marceau.caron@mila.quebec>\n",
    "\n",
    "source: https://github.com/mila-iqia/ivado-mila-dl-school-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLHwvggEZERd"
   },
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKNGtQkkohiM"
   },
   "source": [
    "This tutorial introduces the practical aspects of Deep Learning through the realization of a simple end-to-end project. We will use the deep learning library <a href=\"https://pytorch.org/\"> `PyTorch`</a>, which is well-known for its ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOD70vdvvtin"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nq9FwFVnQihX"
   },
   "source": [
    "## Loading packages and using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reCpBfp1Qcrt"
   },
   "source": [
    "Before we start, we install the necessary packages for the tutorial by using pip. To do this, execute the following cell by selecting it and using `shift+Enter`. This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "c5AlBPjnvzNh",
    "outputId": "48285c9a-dbd0-4585-e493-c089116ae91e"
   },
   "outputs": [],
   "source": [
    "!pip3 install 'torch==1.1.0' 'torchvision==0.3.0' 'Pillow==4.3.0' 'matplotlib==3.0.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djF9gjzLwsDB"
   },
   "source": [
    "Now, import all the modules we will use for this tutorial by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "w9LnNnxBw0wC",
    "outputId": "2423e91f-ab6a-4ad2-db83-948e14bf0f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.3.1\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"GPU Available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKzgFV9Favkt"
   },
   "source": [
    "## PyTorch in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vrus_-F0avkt"
   },
   "source": [
    "*PyTorch* is a Python library that supports a vibrant ecosystem of tools and libraries for ML in vision, NLP, and more. It provides two high-level features:\n",
    "<ul>\n",
    "<li> operations on <a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\">tensors</a> (such as NumPy) with GPU support,</li>\n",
    "<li> operations for creating and optimizing computational graphs with an automatic differentiation system called <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\">Autograd</a>.</li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"https://pytorch.org/docs/stable/torch.html\">PyTorch docs</a> contain the API documentation and <a href=\"https://pytorch.org/tutorials/\">many tutorials</a>.\n",
    "Also, PyTorch offers several data processing utilities. One of these utilities is the class <a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> which offers an easy to use interface to handle a data set. For more information, please refer to the following urls: \n",
    "<ul>\n",
    "<li>PyTorch data sets: <a href=\"http://pytorch.org/docs/master/data.html\"> PyTorch - datasets</a>.</li>\n",
    "<li>A tutorial for loading data: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> PyTorch - data loading tutorial</a>.</li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a package that provides the same functions as CPU tensors but for  CUDA tensors, which are used for GPU computing. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns a boolean indicating if CUDA is currently available. Finally, we recommend using a `device` variable that identifies the device on which you want to perform computations. We can assign a tensor to a device with the method `.to(device)`. By default, the tensors are CPU tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qm122vNmq92L"
   },
   "source": [
    "## Ingredients for a proof of concept (POC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqvhR0ebavmE"
   },
   "source": [
    "To realize a ML POC, you need:\n",
    "<ul>\n",
    "<li>a task description as well as data to support it,</li>\n",
    "<li>an evaluation metric to assess the performance of a model,</li>\n",
    "<li>a model description,</li>\n",
    "<li>a cost function to be optimized,</li>\n",
    "<li>an optimizer that adjusts the parameters of the model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8_pfpu2f6AO"
   },
   "source": [
    "# How to prepare the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5piZxYUhSzq"
   },
   "source": [
    "Our task is to predict whether or not a passenger survived the sinking of the Titanic based on passenger data only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4GuYNDFavlU"
   },
   "source": [
    "## Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiOJx2ytavlU"
   },
   "source": [
    "We can download the Titanic dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
    "This dataset provides information on the fate of 1309 passengers of the first and only journey of the liner \"RMS Titanic\", summarized by economic status (class), gender, age, family information, and survival. The Kaggle platform also uses this dataset as an introduction to classical machine learning. Here, we use it to introduce more advanced concepts related to PyTorch and Deep Learning.\n",
    "\n",
    "We use the library <a href=\"https://pandas.pydata.org/\">Pandas</a> to load the dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "bX_RSiffavlW",
    "outputId": "65feb3e0-c4e7-4812-e918-3767946f865d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv(\n",
    "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
    "    sep='\\t', \n",
    "    index_col=None, \n",
    "    na_values=['NA']\n",
    ")\n",
    "\n",
    "# a snapshot of the first 5 data points\n",
    "titanic_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj88WmCmavlf"
   },
   "source": [
    "**The meaning of the different columns (features) is as follows**:\n",
    "\n",
    "<ol>\n",
    "\n",
    "  <li> <b>pclass</b>: Passenger class (1 = first; 2 = second; 3 = third) </li>\n",
    "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
    "  <li> <b>name</b>: Name </li>\n",
    "  <li> <b>sex</b>: Sex </li>\n",
    "  <li> <b>age</b>: Age </li>\n",
    "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
    "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
    "  <li> <b>ticket</b>: Ticket number </li>\n",
    "  <li> <b>fare</b>: Passenger fare </li>\n",
    "  <li> <b>cabin</b>: Cabin number </li>\n",
    "  <li> <b>embarked</b>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
    "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
    "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
    "  <li> <b>home.dest</b>: the passenger's destination </li>\n",
    " </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2ed5fozqjce"
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__vcZhPnavlg"
   },
   "source": [
    "### Feature selection\n",
    "Some features are not relevant to the task, for example:\n",
    "<ol>\n",
    "  <li> <b>name</b>: Name </li>\n",
    "  <li> <b>ticket</b>: Ticket number </li>\n",
    "  <li> <b>cabin</b>: Cabin number </li>\n",
    "  <li> <b>home.dest</b>: Passenger's destination </li>\n",
    " </ol>\n",
    " \n",
    "Other features give away the label to be predicted and including them would be cheating:\n",
    "<ol>\n",
    "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
    "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "JJ0--SDpavlg",
    "outputId": "bb15e866-2779-4e6f-9deb-4eedf70248ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass_1  pclass_2  pclass_3  sex_female  sex_male      age  \\\n",
       "0         1         1         0         0           1         0  29.0000   \n",
       "1         1         1         0         0           0         1   0.9167   \n",
       "2         0         1         0         0           1         0   2.0000   \n",
       "3         0         1         0         0           0         1  30.0000   \n",
       "4         0         1         0         0           1         0  25.0000   \n",
       "\n",
       "   sibsp  parch      fare  embarked_C  embarked_Q  embarked_S  \n",
       "0      0      0  211.3375           0           0           1  \n",
       "1      1      2  151.5500           0           0           1  \n",
       "2      1      2  151.5500           0           0           1  \n",
       "3      1      2  151.5500           0           0           1  \n",
       "4      1      2  151.5500           0           0           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_preprocess_df = pd.read_csv(\n",
    "    'https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true', \n",
    "    sep=',', \n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "titanic_preprocess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MckYm0M_xhR"
   },
   "source": [
    " ### Feature encoding\n",
    " \n",
    "Some features are **categorical variables**, which means that they can take a finite number of values.\n",
    " <ol>\n",
    "  <li> <b>pclass</b>: Passenger Class </li>\n",
    "  <li> <b>sex</b>: Sex </li>\n",
    "  <li> <b>embarked</b>: Port of embarkation </li>\n",
    " </ol>\n",
    "\n",
    "To process categorical variables, we need to encode them in a way that does not imply an arbitrary order such as using natural numbers (e.g., 1, 2, 3). <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">One-hot encoding</a> is a way to achieve it. We can download the pre-processed dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true.\n",
    "<br>The meaning of the encoded variables is as follows:\n",
    "\n",
    "<ol>\n",
    "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
    "  <li> <b>pclass_1</b>: (1 if passenger in first class; 0 if not) </li>\n",
    "  <li> <b>pclass_2</b>: (1 if passenger in second class; 0 if not) </li>\n",
    "  <li> <b>pclass_3</b>: (1 if passenger in third class; 0 if not) </li>\n",
    "  <li> <b>sex_female</b>: (1 if passenger is female; 0 if not) </li>\n",
    "  <li> <b>sex_male</b>: (1 if passenger is male; 0 if not) </li>\n",
    "  <li> <b>age</b>: Age </li>\n",
    "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
    "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
    "  <li> <b>fare</b>: Passenger fare </li>\n",
    "  <li> <b>embarked_C</b>: (1 if Port of embarkation = Cherbourg (C); 0 otherwise) </li> \n",
    "  <li> <b>embarked_Q</b>: (1 if Port of embarkation = Queenstown (Q); 0 otherwise) </li> \n",
    "  <li> <b>embarked_S</b>: (1 if Port of embarkation = Southampton (S); 0 otherwise)</li> \n",
    " </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJcs6PUTavlm"
   },
   "source": [
    "## Train / validation / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bjbgvffmavlo"
   },
   "source": [
    "At this point, we need to divide the dataset into three subsets:\n",
    "\n",
    "<ol>\n",
    "<li> <b> Train</b> (usually 60% of the dataset): used to train the classification model. </li>   \n",
    "<li> <b> Validation</b> (generally 20% of the dataset): used to evaluate hyper-parameters on held-out data. </li>   \n",
    "<li> <b> Test</b> (usually 20% of the dataset): used to evaluate the generalization performance of the chosen model on held-out data. </li>\n",
    "</ol>\n",
    "\n",
    "We use the <a href=\"https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.split.html\">numpy.split function</a> to separate our dataset into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBmL8VBOavlo"
   },
   "outputs": [],
   "source": [
    "train, validate, test = np.split(\n",
    "    titanic_preprocess_df.sample(frac=1, random_state=seed), \n",
    "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])   # take the 2 indices of the split to have 3 splits.\n",
    "\n",
    "# Remove the label column from X and create a label vectors.\n",
    "X_train = train.drop(['survived'], axis=1).values\n",
    "y_train = train['survived'].values\n",
    "\n",
    "X_val = validate.drop(['survived'], axis=1).values\n",
    "y_val = validate['survived'].values\n",
    "\n",
    "X_test = test.drop(['survived'], axis=1).values\n",
    "y_test = test['survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv74TbIWavlr"
   },
   "source": [
    "## Datasets in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_LJtG-Xavlt"
   },
   "source": [
    "We will use the subclass <b><a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.TensorDataset`</a> </b> to manipulate together the features and targets of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JtT4tV7avlt"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obEPHnlTavkc"
   },
   "source": [
    "# How to define the learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhN5GL6Gavks"
   },
   "source": [
    "A multilayer perceptron (MLP) is a simple computational graph composed of \"hidden layers,\" which are defined by two modules: a *linear transformation* followed by a *non-linearity*. The result of a hidden layer is a vector called *a distributed representation* where each component is associated with a hidden unit.\n",
    "\n",
    "To train this model, we need to define:\n",
    "<ul>\n",
    "<li>the network architecture by choosing the non-linear function and the number of hidden units per layer, </li>\n",
    "<li>the cost function and optimizer. </li>\n",
    "</ul>\n",
    "\n",
    "To solve our task, we will use a MLP with the following properties:\n",
    " <ul>\n",
    " <li> the input dimension of the model is 12,</li>\n",
    " <li> the output dimension of the model is 2,</li>\n",
    " <li> the first dimension of the output is the probability of death and the second dimension is the probability of survival,</li>\n",
    "  <li> the number of hidden layers is 3, </li>\n",
    " <li> the dimensions of the hidden layers are 20, 40, 20 respectively, </li>\n",
    " <li> the activation function is a ReLu for all hidden layers. </li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "701t0e-ravkr"
   },
   "source": [
    "## How to define a model in PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4F5cyijavkv"
   },
   "source": [
    "The <a href=\"https://pytorch.org/docs/stable/nn.html\">PyTorch NN package</a> contains many useful classes for creating computation graphs.\n",
    "<ul>\n",
    "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#module\">torch.nn.Module</a>: \n",
    "any new module must inherit from this class or its descendants (subclasses).\n",
    "</li>   \n",
    "<li> The `forward` method:  any class defining a module must implement the `forward(...)` method, which defines the transformation of inputs to outputs.</li>  \n",
    "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a>: this class implements a linear transformation. By default, it takes two parameters: \n",
    "    <ul>\n",
    "    <li>`in_features`: the size of the data at the input of the module. </li>\n",
    "    <li>`out_features`: the size of the data at the output of the module. </li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li> The module <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">`torch.nn.functional`</a>: \n",
    "it defines a set of functions that can be applied directly to any tensor. As examples, we have:\n",
    "    <ul>\n",
    "    <li> non-linear functions: sigmoid(...), tanh(...), relu(...), etc...</li> \n",
    "    <li> cost functions: mse_loss(...), nll(...., cross_entropy(...), etc ... </li> \n",
    "    <li> regularization functions: droupout(...), etc ... </li> \n",
    "    <li> ...</li> \n",
    "    </ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tscha6S-KIBB"
   },
   "source": [
    "You need to complete the following methods:\n",
    "<ul>\n",
    "<li>The `__init__` method that defines the layers. </li>\n",
    "<li>The `forward(input)` method that returns the `output`. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NyQGwC-avkw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xR5eBfIbavk0"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 20)     # start with 12 features\n",
    "        self.fc2 = nn.Linear(20, 40)\n",
    "        self.fc3 = nn.Linear(40, 20)\n",
    "        self.fc4 = nn.Linear(20, 2)\n",
    "        # nn.module just define the architecture\n",
    "        # F.module do computation directly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvLnHRZ5avk2"
   },
   "source": [
    "## Making predictions with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEXgJMDDavk3"
   },
   "source": [
    "Now, we are ready to test our neural network on randomly selected data.\n",
    "\n",
    "In PyTorch, a model has two different modes:\n",
    "    <ul>\n",
    "    <li> <b>train</b>: used during training, </li>\n",
    "    <li> <b>eval</b>: used during inference for model evaluation. </li>\n",
    "    </ul>\n",
    "The distinction is important since some modules behave differently according to this mode.\n",
    "We will use the <b>eval</b> mode in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "gzcABMezavk6",
    "outputId": "d65d2777-ee2a-4f9d-9259-edb76428ad6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5093, 0.4907],\n",
      "        [0.5069, 0.4931],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5022, 0.4978],\n",
      "        [0.4427, 0.5573]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "neural_net = NeuralNet()\n",
    "neural_net = neural_net.to(device)\n",
    "\n",
    "# Eval mode activation\n",
    "neural_net = neural_net.eval()\n",
    "\n",
    "# Select the first 5 data points\n",
    "data, target = val_dataset[0:5]\n",
    "data = data.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "# Forward propagation of the data through the model\n",
    "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
    "\n",
    "# Convert the logits into probabilities with softmax function\n",
    "output_proba = F.softmax(output, dim=1)\n",
    "\n",
    "# Printing the probability\n",
    "print(output_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVep0BElavlS"
   },
   "source": [
    "The rows define the output of the network, in terms of probabilities over two classes: <b>deceased</b> (first column) or <b>survived</b> (second column), for each of the five input data points. Let us take the label with maximum probability as the predicted label and compare it to the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "_jV4No36qjdU",
    "outputId": "995f5958-ab4d-4c8a-e486-f303405e1e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction\n",
      "tensor([0, 0, 0, 0, 1])\n",
      "Actual data\n",
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Printing predictions (class with the highest probability)\n",
    "_, prediction = torch.max(output_proba, dim=1)     #'_': values\n",
    "\n",
    "print('Model prediction')\n",
    "print(prediction)\n",
    "\n",
    "# Printing the real labels\n",
    "print(\"Actual data\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEIIjqOuqjdc"
   },
   "source": [
    "**Questions**\n",
    "\n",
    "1.   What would be a good way to measure the model performances?\n",
    "2.   How does our model perform?\n",
    "3.   Considering that the model is not trained on the dataset, do you see any problem with your selected measure?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTTlBikYwb-w"
   },
   "source": [
    "\n",
    "**Possible answers**:\n",
    "\n",
    "\n",
    "1.   A simple measure of performance is the accuracy, the number of correct predictions over the total number of examples.\n",
    "2.   This model has an accuracy of 4/5 = 80%.\n",
    "3.   With accuracy, we already have a score of 80% with random weight, which happens when labels are unbalanced, and the model predicts the dominant class by chance. In this section, we are only using five examples to evaluate the model, which is not enough. However, if we observe this behavior on the whole validation set, then we may consider another measure of performance (e.g., precision/recall or F1-score).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uySA2TCavmD"
   },
   "source": [
    "## Define the cost function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkoobCLMavmE"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkX7uSXQavmF"
   },
   "source": [
    "We define the cost function according to the task we want to achieve.\n",
    "\n",
    "PyTorch offers <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">many ready-to-use cost functions</a>.\n",
    "\n",
    "For classification problems, the usual cost function is <b>cross-entropy</b>, and this is the one we will use in this tutorial. In PyTorch, it is defined by the function <a href=\"https://pytorch.org/docs/master/nn.functional.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a>.  Cross entropy allows comparing a $p$ distribution with a reference distribution $t$. It attains its minimum when $t=p$. Its formula for calculating it between the prediction and the target is: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ the examples and $j$ the classes of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHnfYeS5avmF"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cost_function(prediction, target):\n",
    "    loss = F.cross_entropy(prediction, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vsx_cv9Wqjdj"
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hcZaIKtavmH"
   },
   "source": [
    "In Pytorch, thanks to the automatic differentiation mechanism <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, it is possible to automatically calculate the gradient of the cost function and backpropagate it through the computational graph.\n",
    "\n",
    "To do this, we only have to call the method `backward()` on the variable returned by the cost function, e.g., with\n",
    "\n",
    "`loss = cost_function(....)` <br>\n",
    "`loss.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YNo_ymYavmH"
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4AlX9TwavmH"
   },
   "source": [
    "PyTorch provides a <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">set of optimization methods (`torch.optim`)</a> commonly used by the deep learning community. These methods include the following: \n",
    "<ul>\n",
    "<li><b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a></li>\n",
    "<li><b>Adam</b> (Adaptive Moment Estimation): a variant of the gradient descent method in which the learning rate is adjusted for each parameter. This adjustment is based on the estimation of the first and second moments of the gradients. This optimizer has demonstrated excellent performance compared to SGD on many benchmarks. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uam-a0_0qjdl"
   },
   "source": [
    "To be able to use an optimizer in PyTorch, we must instantiate it by passing the following elements:\n",
    "<ul>\n",
    "<li><b>The parameters of the model</b>: these are obtained using the method <b>parameters()</b> on the instantiated model. </li>\n",
    "<li><b>The learning rate (lr)</b>: this is the learning rate to be used to update parameters during the optimization process. </li>\n",
    "<li>There may be other parameters specific to the chosen optimizer.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jt6_Qr6ravmI"
   },
   "source": [
    "PyTorch offers a simplified interface to interact with any optimizer:\n",
    "<ul>\n",
    "<li><b>zero_grad()</b>: Allows to reinitialize the gradients to zero at the beginning of an optimization step.</li>\n",
    "<li><b>step()</b>: Allows to perform an optimization step after a gradient backpropagation step.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZ-lKExqavmI"
   },
   "source": [
    "We will use Adam with a lr of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDMOziJTavmI"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnFOAfdGqjdr"
   },
   "source": [
    "# How to train a model?\n",
    "First, we need some definitions:\n",
    "<ol>\n",
    "<li>\n",
    "<b>Epoch</b>: a complete pass over the entire training dataset.\n",
    "</li>\n",
    "<li>\n",
    "<b>Iteration</b>: an update of the model parameters. Many iterations can occur before the end of an epoch.\n",
    "</li>\n",
    "<li>\n",
    "<b>Mini-batch</b>: A subset of training data used to estimate the average of gradients. In other words, at each iteration, a mini-batch is used. \n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLXjNiDTavmK"
   },
   "source": [
    "## Creating the mini-batches\n",
    "PyTorch offers a utility called <b><a href=\"http://pytorch.org/docs/master/data.html\"> torch.utils.data.DataLoader </a></b> to load any dataset and automatically split it into mini-batches. During training, the data presented to the network should appear in a different order from one epoch to another. We will prepare the `DataLoader` for our three datasets (training, validation, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGoQZSdqavmM"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 32  # number of data in a training batch.\n",
    "eval_batch_size = 32   # number of data in an batch.\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia3ai-GvavmP"
   },
   "source": [
    "## Simple training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9wNZrTnavmQ"
   },
   "source": [
    "Here we define our training procedure for an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyK9xCsZavmR"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, train_loader, optimizer, device):\n",
    "    \n",
    "    # activate the training mode\n",
    "    model.train()\n",
    "    \n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # iteration over the mini-batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # transfer the data on the chosen device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # reinitialize the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward propagation on the data\n",
    "        prediction = model(data)\n",
    "        \n",
    "        # compute the cost function w.r.t. the targets\n",
    "        loss = cost_function(prediction, target)\n",
    "        \n",
    "        # execute the backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # execute an optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate the loss\n",
    "        total_loss += loss.item()*len(data)\n",
    "        \n",
    "        # compute the number of correct predictions\n",
    "        _, pred_classes = torch.max(prediction, dim=1)        \n",
    "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
    "         \n",
    "        \n",
    "    # compute the average cost per epoch\n",
    "    mean_loss = total_loss/len(train_loader.dataset)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
    "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
    "        100. * acc))   \n",
    "    \n",
    "    # return the average loss and the accuracy\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxG666rmavmU"
   },
   "source": [
    "## Evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGexbWaHavmU"
   },
   "source": [
    "Here we define our model evaluation procedure.\n",
    "<br/>\n",
    "In addition to switching the model to **eval** mode, it is essential to disable the gradient calculation. \n",
    "<br/>\n",
    "To do this, PyTorch offers a set of context managers to <a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">locally disable/enable gradient calculation </a>:\n",
    "<ol>\n",
    "<li>\n",
    "`torch.no_grad()`: disable gradient calculation.\n",
    "</li>\n",
    "<li>\n",
    "`torch.enable_grad()`: enable gradient calculation.\n",
    "</li>\n",
    "<li>\n",
    "`torch.set_grad_enabled(bool)`: enable/disable gradient calculation.\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gQj9W5LavmU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, device):\n",
    "    \n",
    "    # activate the evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # iterate over the batches\n",
    "        for batch_idx, (data, target) in enumerate(eval_loader):\n",
    "\n",
    "            # transfer the data on the chosen device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward propagation on the data\n",
    "            prediction = model(data)\n",
    "\n",
    "            # compute the cost function w.r.t. the targets\n",
    "            loss = cost_function(prediction, target)           \n",
    "\n",
    "\n",
    "            # accumulate the loss\n",
    "            total_loss += loss.item()*len(data)\n",
    "\n",
    "            # compute the number of correct predictions en sortie)\n",
    "            _, pred_classes = torch.max(prediction, dim=1) \n",
    "            correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
    "          \n",
    "    \n",
    "    # compute the average cost per epoch\n",
    "    mean_loss = total_loss/len(eval_loader.dataset)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    acc = correct / len(eval_loader.dataset)\n",
    "        \n",
    "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
    "        mean_loss, correct, len(eval_loader.dataset),\n",
    "        100. * acc)) \n",
    "    \n",
    "    # return the average loss and the accuracy\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMUyZNxdavmW"
   },
   "source": [
    "## Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQLklQXAavmW"
   },
   "source": [
    "For training phases that require much time, it is recommended to save periodically the model parameters. This step is commonly referred to as <b>checkpointing</b>.\n",
    "\n",
    "PyTorch offers <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">a simple mechanism</a> to perform this operation. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ld-Y2gF-avmX"
   },
   "source": [
    "We implement two methods here:\n",
    "<ul>\n",
    "<li> the first one for <b> saving </b> a model,</li>\n",
    "<li> the second for <b> loading </b> a model checkpoint. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMmNpma2avmX"
   },
   "outputs": [],
   "source": [
    "def save_model(epoch, model, path='./'):\n",
    "    \n",
    "    # creating the file name indexed by the epoch value\n",
    "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
    "    \n",
    "    # saving the model parameters\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZptgqQRavmZ"
   },
   "outputs": [],
   "source": [
    "def load_model(epoch, model, path='./'):\n",
    "    \n",
    "    # creating the file name indexed by the epoch value\n",
    "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
    "    \n",
    "    # loading the parameters of the saved model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve8sOocWavma"
   },
   "source": [
    "It is also possible to save the status of the optimizer in PyTorch, which is very important when we want to resume training the model from a given backup. For more information, please consult <a href='https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3'>the following URL</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lcAP8-1avma"
   },
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "keMpyePsavmb",
    "outputId": "638ef04a-0f0e-4e8a-8b50-23884324a79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1   Avg_Loss: 0.82669   Acc: 275/625 (44.000%)\n",
      "Eval:  Avg_Loss: 0.69597   Acc: 99/209 (47.368%)\n",
      "Train Epoch: 2   Avg_Loss: 0.66111   Acc: 397/625 (63.520%)\n",
      "Eval:  Avg_Loss: 0.65175   Acc: 144/209 (68.900%)\n",
      "Train Epoch: 3   Avg_Loss: 0.63141   Acc: 421/625 (67.360%)\n",
      "Eval:  Avg_Loss: 0.62352   Acc: 145/209 (69.378%)\n",
      "Train Epoch: 4   Avg_Loss: 0.61599   Acc: 423/625 (67.680%)\n",
      "Eval:  Avg_Loss: 0.61681   Acc: 142/209 (67.943%)\n",
      "Train Epoch: 5   Avg_Loss: 0.61261   Acc: 431/625 (68.960%)\n",
      "Eval:  Avg_Loss: 0.60453   Acc: 142/209 (67.943%)\n",
      "Train Epoch: 6   Avg_Loss: 0.61117   Acc: 426/625 (68.160%)\n",
      "Eval:  Avg_Loss: 0.60108   Acc: 144/209 (68.900%)\n",
      "Train Epoch: 7   Avg_Loss: 0.60210   Acc: 427/625 (68.320%)\n",
      "Eval:  Avg_Loss: 0.59878   Acc: 143/209 (68.421%)\n",
      "Train Epoch: 8   Avg_Loss: 0.60072   Acc: 427/625 (68.320%)\n",
      "Eval:  Avg_Loss: 0.59606   Acc: 144/209 (68.900%)\n",
      "Train Epoch: 9   Avg_Loss: 0.59455   Acc: 429/625 (68.640%)\n",
      "Eval:  Avg_Loss: 0.58268   Acc: 145/209 (69.378%)\n",
      "Train Epoch: 10   Avg_Loss: 0.58700   Acc: 429/625 (68.640%)\n",
      "Eval:  Avg_Loss: 0.57752   Acc: 146/209 (69.856%)\n",
      "Train Epoch: 11   Avg_Loss: 0.57737   Acc: 435/625 (69.600%)\n",
      "Eval:  Avg_Loss: 0.56965   Acc: 146/209 (69.856%)\n",
      "Train Epoch: 12   Avg_Loss: 0.56613   Acc: 445/625 (71.200%)\n",
      "Eval:  Avg_Loss: 0.56516   Acc: 146/209 (69.856%)\n",
      "Train Epoch: 13   Avg_Loss: 0.55887   Acc: 454/625 (72.640%)\n",
      "Eval:  Avg_Loss: 0.55602   Acc: 151/209 (72.249%)\n",
      "Train Epoch: 14   Avg_Loss: 0.55907   Acc: 456/625 (72.960%)\n",
      "Eval:  Avg_Loss: 0.56250   Acc: 147/209 (70.335%)\n",
      "Train Epoch: 15   Avg_Loss: 0.54461   Acc: 462/625 (73.920%)\n",
      "Eval:  Avg_Loss: 0.53498   Acc: 152/209 (72.727%)\n",
      "Train Epoch: 16   Avg_Loss: 0.52883   Acc: 471/625 (75.360%)\n",
      "Eval:  Avg_Loss: 0.52431   Acc: 156/209 (74.641%)\n",
      "Train Epoch: 17   Avg_Loss: 0.50891   Acc: 478/625 (76.480%)\n",
      "Eval:  Avg_Loss: 0.52312   Acc: 157/209 (75.120%)\n",
      "Train Epoch: 18   Avg_Loss: 0.49627   Acc: 483/625 (77.280%)\n",
      "Eval:  Avg_Loss: 0.50566   Acc: 161/209 (77.033%)\n",
      "Train Epoch: 19   Avg_Loss: 0.48700   Acc: 485/625 (77.600%)\n",
      "Eval:  Avg_Loss: 0.51311   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 20   Avg_Loss: 0.49819   Acc: 480/625 (76.800%)\n",
      "Eval:  Avg_Loss: 0.50521   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 21   Avg_Loss: 0.46607   Acc: 498/625 (79.680%)\n",
      "Eval:  Avg_Loss: 0.50268   Acc: 158/209 (75.598%)\n",
      "Train Epoch: 22   Avg_Loss: 0.48540   Acc: 484/625 (77.440%)\n",
      "Eval:  Avg_Loss: 0.50149   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 23   Avg_Loss: 0.47134   Acc: 483/625 (77.280%)\n",
      "Eval:  Avg_Loss: 0.50674   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 24   Avg_Loss: 0.46481   Acc: 492/625 (78.720%)\n",
      "Eval:  Avg_Loss: 0.50154   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 25   Avg_Loss: 0.46043   Acc: 494/625 (79.040%)\n",
      "Eval:  Avg_Loss: 0.49345   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 26   Avg_Loss: 0.44816   Acc: 496/625 (79.360%)\n",
      "Eval:  Avg_Loss: 0.49766   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 27   Avg_Loss: 0.44127   Acc: 498/625 (79.680%)\n",
      "Eval:  Avg_Loss: 0.50072   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 28   Avg_Loss: 0.44017   Acc: 507/625 (81.120%)\n",
      "Eval:  Avg_Loss: 0.49737   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 29   Avg_Loss: 0.43723   Acc: 502/625 (80.320%)\n",
      "Eval:  Avg_Loss: 0.51149   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 30   Avg_Loss: 0.43729   Acc: 506/625 (80.960%)\n",
      "Eval:  Avg_Loss: 0.50325   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 31   Avg_Loss: 0.44262   Acc: 501/625 (80.160%)\n",
      "Eval:  Avg_Loss: 0.51388   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 32   Avg_Loss: 0.45369   Acc: 500/625 (80.000%)\n",
      "Eval:  Avg_Loss: 0.50466   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 33   Avg_Loss: 0.45676   Acc: 497/625 (79.520%)\n",
      "Eval:  Avg_Loss: 0.51477   Acc: 160/209 (76.555%)\n",
      "Train Epoch: 34   Avg_Loss: 0.45612   Acc: 497/625 (79.520%)\n",
      "Eval:  Avg_Loss: 0.50982   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 35   Avg_Loss: 0.43095   Acc: 507/625 (81.120%)\n",
      "Eval:  Avg_Loss: 0.50017   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 36   Avg_Loss: 0.42578   Acc: 508/625 (81.280%)\n",
      "Eval:  Avg_Loss: 0.49845   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 37   Avg_Loss: 0.42918   Acc: 507/625 (81.120%)\n",
      "Eval:  Avg_Loss: 0.51367   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 38   Avg_Loss: 0.43003   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.51591   Acc: 162/209 (77.512%)\n",
      "Train Epoch: 39   Avg_Loss: 0.43331   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.52294   Acc: 162/209 (77.512%)\n",
      "Train Epoch: 40   Avg_Loss: 0.42899   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.50765   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 41   Avg_Loss: 0.42353   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.53098   Acc: 160/209 (76.555%)\n",
      "Train Epoch: 42   Avg_Loss: 0.43162   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.51242   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 43   Avg_Loss: 0.42274   Acc: 508/625 (81.280%)\n",
      "Eval:  Avg_Loss: 0.52351   Acc: 161/209 (77.033%)\n",
      "Train Epoch: 44   Avg_Loss: 0.41901   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.51061   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 45   Avg_Loss: 0.41774   Acc: 508/625 (81.280%)\n",
      "Eval:  Avg_Loss: 0.51765   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 46   Avg_Loss: 0.42539   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.49882   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 47   Avg_Loss: 0.42986   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.52549   Acc: 161/209 (77.033%)\n",
      "Train Epoch: 48   Avg_Loss: 0.42133   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.51714   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 49   Avg_Loss: 0.42430   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.51036   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 50   Avg_Loss: 0.41490   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.49750   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 51   Avg_Loss: 0.40705   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.51579   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 52   Avg_Loss: 0.41079   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.51171   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 53   Avg_Loss: 0.40665   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.51348   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 54   Avg_Loss: 0.40773   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.50584   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 55   Avg_Loss: 0.40528   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.51098   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 56   Avg_Loss: 0.40169   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.51603   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 57   Avg_Loss: 0.40443   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.51208   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 58   Avg_Loss: 0.40677   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.51689   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 59   Avg_Loss: 0.40295   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.50207   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 60   Avg_Loss: 0.41354   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.51340   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 61   Avg_Loss: 0.40447   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.51662   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 62   Avg_Loss: 0.40276   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.51596   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 63   Avg_Loss: 0.40671   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.52560   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 64   Avg_Loss: 0.40590   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.54552   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 65   Avg_Loss: 0.41163   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.50450   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 66   Avg_Loss: 0.40486   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.51603   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 67   Avg_Loss: 0.40354   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.53236   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 68   Avg_Loss: 0.40674   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.52243   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 69   Avg_Loss: 0.39861   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.51513   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 70   Avg_Loss: 0.39501   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.51965   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 71   Avg_Loss: 0.40384   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.53967   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 72   Avg_Loss: 0.39575   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.54079   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 73   Avg_Loss: 0.41207   Acc: 503/625 (80.480%)\n",
      "Eval:  Avg_Loss: 0.51060   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 74   Avg_Loss: 0.39802   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.50302   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 75   Avg_Loss: 0.39220   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.52859   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 76   Avg_Loss: 0.39656   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.51436   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 77   Avg_Loss: 0.39145   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.53645   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 78   Avg_Loss: 0.39334   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.53281   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 79   Avg_Loss: 0.39815   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.53625   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 80   Avg_Loss: 0.39783   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.55439   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 81   Avg_Loss: 0.42876   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.55882   Acc: 167/209 (79.904%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 82   Avg_Loss: 0.43476   Acc: 497/625 (79.520%)\n",
      "Eval:  Avg_Loss: 0.53700   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 83   Avg_Loss: 0.40508   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.54264   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 84   Avg_Loss: 0.40325   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.54721   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 85   Avg_Loss: 0.40475   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.52622   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 86   Avg_Loss: 0.39416   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.53282   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 87   Avg_Loss: 0.39871   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.51514   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 88   Avg_Loss: 0.39338   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.53511   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 89   Avg_Loss: 0.38846   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.52646   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 90   Avg_Loss: 0.38506   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.53982   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 91   Avg_Loss: 0.39396   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.53156   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 92   Avg_Loss: 0.39150   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.52424   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 93   Avg_Loss: 0.38880   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.53259   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 94   Avg_Loss: 0.38527   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.54343   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 95   Avg_Loss: 0.38233   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.55508   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 96   Avg_Loss: 0.38985   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.52917   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 97   Avg_Loss: 0.38830   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.54741   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 98   Avg_Loss: 0.38575   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.53059   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 99   Avg_Loss: 0.38476   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.54084   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 100   Avg_Loss: 0.38348   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.55011   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 101   Avg_Loss: 0.39069   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.54557   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 102   Avg_Loss: 0.38530   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.55731   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 103   Avg_Loss: 0.38386   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.55117   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 104   Avg_Loss: 0.38128   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.53140   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 105   Avg_Loss: 0.39057   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.52422   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 106   Avg_Loss: 0.38181   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.54568   Acc: 174/209 (83.254%)\n",
      "Train Epoch: 107   Avg_Loss: 0.37953   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.55121   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 108   Avg_Loss: 0.37725   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.55389   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 109   Avg_Loss: 0.37980   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.54617   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 110   Avg_Loss: 0.38019   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.55920   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 111   Avg_Loss: 0.37404   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.54398   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 112   Avg_Loss: 0.38093   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.55697   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 113   Avg_Loss: 0.37959   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.55691   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 114   Avg_Loss: 0.38245   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.55407   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 115   Avg_Loss: 0.38065   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.55903   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 116   Avg_Loss: 0.37243   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.57284   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 117   Avg_Loss: 0.37707   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.58425   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 118   Avg_Loss: 0.37513   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.55882   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 119   Avg_Loss: 0.37400   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.58551   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 120   Avg_Loss: 0.39284   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.58440   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 121   Avg_Loss: 0.39802   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.57077   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 122   Avg_Loss: 0.37712   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.57876   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 123   Avg_Loss: 0.37784   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.56752   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 124   Avg_Loss: 0.37189   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.58487   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 125   Avg_Loss: 0.37070   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.54698   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 126   Avg_Loss: 0.37901   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.58905   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 127   Avg_Loss: 0.37216   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.55117   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 128   Avg_Loss: 0.37411   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.58049   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 129   Avg_Loss: 0.37149   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.57903   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 130   Avg_Loss: 0.37098   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.59163   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 131   Avg_Loss: 0.38007   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.58830   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 132   Avg_Loss: 0.37207   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.56079   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 133   Avg_Loss: 0.38987   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.58726   Acc: 175/209 (83.732%)\n",
      "Train Epoch: 134   Avg_Loss: 0.37068   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.57925   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 135   Avg_Loss: 0.37191   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.58491   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 136   Avg_Loss: 0.37213   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.57920   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 137   Avg_Loss: 0.37479   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.60298   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 138   Avg_Loss: 0.36875   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.56671   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 139   Avg_Loss: 0.37568   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.60353   Acc: 174/209 (83.254%)\n",
      "Train Epoch: 140   Avg_Loss: 0.39557   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.57456   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 141   Avg_Loss: 0.37618   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.58931   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 142   Avg_Loss: 0.37337   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.56702   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 143   Avg_Loss: 0.36795   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.57470   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 144   Avg_Loss: 0.36902   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.58266   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 145   Avg_Loss: 0.36944   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.59510   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 146   Avg_Loss: 0.37183   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.59384   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 147   Avg_Loss: 0.36679   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.59910   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 148   Avg_Loss: 0.36909   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.58857   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 149   Avg_Loss: 0.36735   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.60124   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 150   Avg_Loss: 0.36595   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.59212   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 151   Avg_Loss: 0.36658   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.57765   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 152   Avg_Loss: 0.36406   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.61539   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 153   Avg_Loss: 0.36377   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.57781   Acc: 174/209 (83.254%)\n",
      "Train Epoch: 154   Avg_Loss: 0.35754   Acc: 533/625 (85.280%)\n",
      "Eval:  Avg_Loss: 0.62580   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 155   Avg_Loss: 0.37638   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.65680   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 156   Avg_Loss: 0.38740   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.60651   Acc: 166/209 (79.426%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 157   Avg_Loss: 0.36717   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.62615   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 158   Avg_Loss: 0.36532   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.56922   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 159   Avg_Loss: 0.36538   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.59799   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 160   Avg_Loss: 0.37126   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.60070   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 161   Avg_Loss: 0.36846   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.61783   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 162   Avg_Loss: 0.36910   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.57509   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 163   Avg_Loss: 0.36566   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.58777   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 164   Avg_Loss: 0.36287   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.64361   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 165   Avg_Loss: 0.38015   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.61008   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 166   Avg_Loss: 0.36384   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.60025   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 167   Avg_Loss: 0.36353   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.61918   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 168   Avg_Loss: 0.35849   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.62625   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 169   Avg_Loss: 0.36186   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.60303   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 170   Avg_Loss: 0.37087   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.60207   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 171   Avg_Loss: 0.36295   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.57974   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 172   Avg_Loss: 0.36285   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.62233   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 173   Avg_Loss: 0.35624   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.58499   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 174   Avg_Loss: 0.35677   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.62142   Acc: 174/209 (83.254%)\n",
      "Train Epoch: 175   Avg_Loss: 0.36560   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.61107   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 176   Avg_Loss: 0.35917   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.62744   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 177   Avg_Loss: 0.35540   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.59999   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 178   Avg_Loss: 0.35736   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.61432   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 179   Avg_Loss: 0.35977   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.63158   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 180   Avg_Loss: 0.35196   Acc: 531/625 (84.960%)\n",
      "Eval:  Avg_Loss: 0.62966   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 181   Avg_Loss: 0.35920   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.62455   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 182   Avg_Loss: 0.35606   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.60746   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 183   Avg_Loss: 0.35499   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.61403   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 184   Avg_Loss: 0.35468   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.59984   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 185   Avg_Loss: 0.35638   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.65437   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 186   Avg_Loss: 0.35103   Acc: 531/625 (84.960%)\n",
      "Eval:  Avg_Loss: 0.64567   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 187   Avg_Loss: 0.36452   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.64006   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 188   Avg_Loss: 0.35943   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.61513   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 189   Avg_Loss: 0.36313   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.64500   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 190   Avg_Loss: 0.35237   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.63239   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 191   Avg_Loss: 0.35721   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.65140   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 192   Avg_Loss: 0.35550   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.63104   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 193   Avg_Loss: 0.35371   Acc: 533/625 (85.280%)\n",
      "Eval:  Avg_Loss: 0.62307   Acc: 173/209 (82.775%)\n",
      "Train Epoch: 194   Avg_Loss: 0.35159   Acc: 531/625 (84.960%)\n",
      "Eval:  Avg_Loss: 0.61827   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 195   Avg_Loss: 0.39451   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.67327   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 196   Avg_Loss: 0.38654   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.70690   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 197   Avg_Loss: 0.35953   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.65103   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 198   Avg_Loss: 0.35620   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.64331   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 199   Avg_Loss: 0.35209   Acc: 533/625 (85.280%)\n",
      "Eval:  Avg_Loss: 0.65693   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 200   Avg_Loss: 0.35662   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.66904   Acc: 170/209 (81.340%)\n",
      "\n",
      "\n",
      "\n",
      "Optimization ended.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maximum number of epoch\n",
    "numEpochs = 200\n",
    "\n",
    "# Saving frequency\n",
    "checkpoint_freq = 10\n",
    "\n",
    "# Directory for data backup\n",
    "path = './'\n",
    "\n",
    "# Accumulators of average costs obtained per epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Performance accumulators per epoch\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Model definition\n",
    "neural_net = NeuralNet()\n",
    "\n",
    "# Load the model on the chosen device\n",
    "neural_net = neural_net.to(device)\n",
    "\n",
    "# Optimizer definition\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
    "# optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "# Learning loop\n",
    "for epoch in range(1, numEpochs + 1):\n",
    "    \n",
    "    # train the model with the train dataset\n",
    "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
    "    \n",
    "    # evaluate the model with the validation dataset\n",
    "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
    "    \n",
    "    # Save the costs obtained\n",
    "    train_losses.append(train_loss)    \n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save the performances\n",
    "    train_accuracies.append(train_acc)    \n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Checkpoint\n",
    "    if epoch % checkpoint_freq ==0:\n",
    "        save_model(epoch, neural_net, path)\n",
    "\n",
    "# Save the model at the end of the training\n",
    "save_model(numEpochs, neural_net, path)\n",
    "    \n",
    "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86OZRLrjavmd"
   },
   "source": [
    "## Interpreting the output of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "mklvQruYavme",
    "outputId": "193eeca0-db5e-477b-818d-7b5e6d883848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9435, 0.0565],\n",
      "        [0.6777, 0.3223],\n",
      "        [0.5894, 0.4106],\n",
      "        [0.5691, 0.4309],\n",
      "        [0.8601, 0.1399],\n",
      "        [0.9518, 0.0482],\n",
      "        [0.8308, 0.1692],\n",
      "        [0.8338, 0.1662],\n",
      "        [0.0085, 0.9915],\n",
      "        [0.0123, 0.9877]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Activate the evaluation mode\n",
    "neural_net = neural_net.eval()\n",
    "\n",
    "# Select the first 10 data points of the validation set\n",
    "data, target = val_dataset[0:10]\n",
    "data = data.to(device)\n",
    "\n",
    "# Executing the neural network\n",
    "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
    "\n",
    "# Transform the output into a probability distribution with a softmax function\n",
    "output_proba = F.softmax(output, dim=1)\n",
    "\n",
    "# Print the probability\n",
    "print(output_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "RvIEqKt0qjeT",
    "outputId": "f1c4e779-e9e1-4873-be1e-1366fb515836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "Targets\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# For each example, retrieve the class with the highest probability.\n",
    "_, prediction = torch.max(output_proba, dim=1)\n",
    "\n",
    "print(\"Model predictions\")\n",
    "print(prediction)\n",
    "\n",
    "print(\"Targets\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V11J3Jihavmy"
   },
   "source": [
    "## Visualizing of the learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9_9C_tXavmz"
   },
   "source": [
    "The visualization of the learning curve allows to detect possible problems that may have occurred during learning, for example, overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "iNcbpl0tavm0",
    "outputId": "fa201aa2-5e25-4701-c0a9-33415773d68b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(len(train_losses)))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
    "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.grid()\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
    "leg.get_frame().set_alpha(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "g-VGQ2pMavm4",
    "outputId": "267e91d4-48e3-413d-e2e2-0feac7335035"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(len(train_accuracies)))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
    "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
    "leg.get_frame().set_alpha(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oktkpkuqjet"
   },
   "source": [
    "**Questions**:\n",
    "\n",
    "*   What do you observe in the previous graphs?\n",
    "*   At which epoch is it interesting to retrieve the model parameters for inference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwhRt39yzug-"
   },
   "source": [
    "**Possible answers:**\n",
    "\n",
    "*   The model is learning since the training loss is decreasing. However, we can observe by comparing the two loss curves (training and validation) that the model begins to overfit at epoch 25 w.r.t. cross-entropy. Nevertheless, the accuracy continues to increase until epoch 100.\n",
    "*   Around epoch 125 where the accuracy on the validation set is maximal. The performance metric is more important than the loss because it is the measure used to determine the best model (i.e., model selection). The training loss is only a surrogate required to generate the feedback for a particular model. Another model could use another loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK_eUsq3avm8"
   },
   "source": [
    "# How to evaluate a model on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UREO5elavm8"
   },
   "source": [
    "We can finally evaluate our model on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pPWvDM-qavm8",
    "outputId": "b39e294a-47fb-401a-ffa4-5d2a407f0980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval:  Avg_Loss: 0.53942   Acc: 164/209 (78.469%)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvP_-KUwqjez"
   },
   "source": [
    "**Questions**:\n",
    "\n",
    "a) Compare validation and test metrics. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov4CKUFh5tun"
   },
   "source": [
    "\n",
    "**Possible answer**: we observe a discrepancy between the two metrics (validation: 81.8%, test: 78.5%). We explain this difference by the small number of examples used in both sets (~ 209 examples). It means that our current estimates can have a variance of few percents. To assess it more precisely, we could use k-fold cross-validation.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_mlp_tags.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
